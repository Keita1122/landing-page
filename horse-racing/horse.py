# ãƒ¬ãƒ¼ã‚¹æƒ…å ±
id = 202509030411  # å®å¡šè¨˜å¿µ
race_url = "https://race.netkeiba.com/race/shutuba.html?race_id=" + str(id) + "&rf=race_submenu"

# import
import re
from bs4 import BeautifulSoup
import requests
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from math import isnan
import pyperclip
from io import StringIO


# ãƒ¬ãƒ¼ã‚¹æƒ…å ±å–å¾—
response = requests.get(race_url, headers={'User-Agent': 'Mozilla/5.0'})
soup = BeautifulSoup(response.content, "html.parser")

# ç«¶èµ°é¦¬æˆç¸¾urlã‚’å–å¾—
urls = [cell.find("a")["href"] for cell in soup.select("td.HorseInfo")]

# ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—
race_name = soup.find('h1', class_='RaceName').text.strip()
race_info = soup.find('div', class_='RaceData01').find_all('span')[0].text.strip()
race_type = ''.join(filter(str.isalpha, race_info.split('m')[0]))
distance = ''.join(filter(str.isdigit, race_info))
track_condition = soup.find('div', class_='RaceData01').text.split('é¦¬å ´:')[1].split()[0].strip()
track_name = soup.find('div', class_='RaceData02').find_all('span')[1].text

# å‡ºé¦¬è¡¨å–å¾—
table_data = []
table = soup.find('table', {'class': 'Shutuba_Table'})
if table:
    rows = table.find_all('tr')
    for row in rows:
        row_data = []
        cols = row.find_all('td')
        for col in cols:
            row_data.append(col.get_text(strip=True))
        table_data.append(row_data)

    table_data = table_data[2:]  # 0,1è¡Œç›®ã®ã‚´ãƒŸã‚’å‰Šé™¤

    # ä¸è¦åˆ—å‰Šé™¤ï¼ˆå°ã€ã‚ªãƒƒã‚ºãªã©ï¼‰
    for row in table_data:
        if len(row) > 2:
            del row[2]
        if len(row) > 6:
            del row[6:]

    # pandaså¤‰æ›
    headers = ["æ ", "é¦¬ç•ª", "é¦¬å", "æ€§é½¢", "æ–¤é‡", "é¨æ‰‹"]
    df = pd.DataFrame(table_data, columns=headers)

    # å¹³å‡æ–¤é‡ã®è¨ˆç®—
    df['æ–¤é‡'] = pd.to_numeric(df['æ–¤é‡'], errors='coerce')
    average_weight = df['æ–¤é‡'].mean()

    # å‡ºé¦¬è¡¨è¡¨ç¤ºï¼ˆå¿…è¦ã«å¿œã˜ã¦æœ‰åŠ¹åŒ–ï¼‰
    print(df)

    for index in range(len(urls)):
        horse_name = df["é¦¬å"][index]
        print(f"\nğŸ é¦¬å: {horse_name}")

        relative_url = urls[index]
        horse_url = relative_url if relative_url.startswith("http") else "https://db.netkeiba.com" + relative_url

        try:
            race_results_list_response = requests.get(horse_url, headers={'User-Agent': 'Mozilla/5.0'})
            race_results_list_soup = BeautifulSoup(race_results_list_response.content, "html.parser")
            race_results_table = race_results_list_soup.find('table', {'class': 'db_h_race_results'})

            if race_results_table:
                html_str = str(race_results_table)
                race_results = pd.read_html(StringIO(html_str), encoding='utf-8')[0]

                # åˆ—åç¢ºèªï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                print("ğŸ“ åˆ—å:", list(race_results.columns))

                # å­˜åœ¨ã™ã‚‹åˆ—ã ã‘ã‚’é¸æŠã™ã‚‹ã‚ˆã†ã«å¤‰æ›´
                cols_to_show = [col for col in ["æ—¥ä»˜", "é–‹å‚¬", "ãƒ¬ãƒ¼ã‚¹å", "ç€é †", "è·é›¢", "é¦¬å ´", "ã‚¿ã‚¤ãƒ ", "é¨æ‰‹"] if col in race_results.columns]

                print("ğŸ“‹ æœ€æ–°5èµ°:")
                print(race_results[cols_to_show].head(5))
            else:
                print("âš ï¸ æˆç¸¾ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

        except Exception as e:
            print(f"âš ï¸ å–å¾—å¤±æ•—: {e}")
else:
    print("å‡ºé¦¬è¡¨(Shutuba_Table class)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")